# hyperparameter_tuning.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import scipy.sparse
import pickle
import os
import time
import warnings
import json
warnings.filterwarnings('ignore')

print("="*60)
print("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†ææ¨¡å‹ - è¶…å‚æ•°è°ƒä¼˜")
print("="*60)

# 1. åŠ è½½æ•°æ®
data_dir = './data/pandas_processed'
X = scipy.sparse.load_npz(os.path.join(data_dir, 'X_tfidf_features.npz'))
y = pd.read_pickle(os.path.join(data_dir, 'y_labels.pkl'))

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (ä¸ä¹‹å‰ä¸€è‡´)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ¡æ ·æœ¬")
print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ¡æ ·æœ¬")
print(f"ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")

# 2. é€‰æ‹©å¹¶å®šä¹‰å¾…è°ƒä¼˜çš„åŸºæ¨¡å‹
# æ ¹æ®ä¹‹å‰çš„ç»“æœï¼Œé€‰æ‹©é€»è¾‘å›å½’æˆ–LinearSVCã€‚è¿™é‡Œä»¥é€»è¾‘å›å½’ä¸ºä¾‹ï¼Œå®ƒé€šå¸¸æ›´é«˜æ•ˆç¨³å®šã€‚
print("\n" + "-"*40)
print("é€‰æ‹©é€»è¾‘å›å½’ (Logistic Regression) è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
print("-"*40)

base_model = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)

# 3. å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
# C: æ­£åˆ™åŒ–å¼ºåº¦ï¼Œè¶Šå°æ­£åˆ™åŒ–è¶Šå¼ºã€‚
# penalty: æ­£åˆ™åŒ–ç±»å‹ã€‚æ³¨æ„ï¼š'l1'æ­£åˆ™åŒ–éœ€è¦solveræ”¯æŒï¼ˆå¦‚'saga'æˆ–'liblinear')ã€‚
# solver: ä¼˜åŒ–ç®—æ³•ã€‚
param_grid = [
    {
        'C': [0.01, 0.1, 1, 10, 100],  # è¦†ç›–ä»å¼ºæ­£åˆ™åŒ–åˆ°å¼±æ­£åˆ™åŒ–çš„èŒƒå›´
        'penalty': ['l2'],
        'solver': ['lbfgs', 'saga']  # 'lbfgs'æ˜¯é»˜è®¤ï¼Œå¯¹l2é«˜æ•ˆï¼›'saga'é€šç”¨æ€§å¼º
    },
    {
        'C': [0.01, 0.1, 1, 10],
        'penalty': ['l1'],
        'solver': ['saga', 'liblinear'],  # æ”¯æŒl1æ­£åˆ™åŒ–çš„æ±‚è§£å™¨
        'max_iter': [2000]  # l1å¯èƒ½éœ€è¦æ›´å¤šè¿­ä»£
    }
]

print(f"è¶…å‚æ•°ç»„åˆæ€»æ•°: {sum(len(d) for d in param_grid)}")
print("å¼€å§‹ç½‘æ ¼æœç´¢... (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)")

# 4. åˆå§‹åŒ–GridSearchCV
# ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ï¼Œä»¥F1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    scoring='f1_weighted',  # ä½¿ç”¨åŠ æƒF1åˆ†æ•°ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®æ›´å‹å¥½
    cv=5,                   # 5æŠ˜äº¤å‰éªŒè¯
    verbose=1,              # è¾“å‡ºè¯¦ç»†è¿›åº¦
    n_jobs=-1               # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒå¹¶è¡Œè®¡ç®—
)

# 5. æ‰§è¡Œç½‘æ ¼æœç´¢ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„éƒ¨åˆ†ï¼‰
start_time = time.time()
grid_search.fit(X_train, y_train)
end_time = time.time()

print(f"\nç½‘æ ¼æœç´¢å®Œæˆï¼è€—æ—¶: {end_time - start_time:.2f} ç§’")

# 6. è¾“å‡ºæœ€ä½³å‚æ•°å’Œäº¤å‰éªŒè¯ç»“æœ
print("\n" + "="*40)
print("æœ€ä½³è¶…å‚æ•°ç»„åˆ")
print("="*40)
best_params = grid_search.best_params_
print(json.dumps(best_params, indent=4))
print(f"\næœ€ä½³äº¤å‰éªŒè¯F1åˆ†æ•°: {grid_search.best_score_:.4f}")

# æŸ¥çœ‹æ‰€æœ‰å‚æ•°ç»„åˆçš„ç»“æœ
cv_results_df = pd.DataFrame(grid_search.cv_results_)
cv_results_df = cv_results_df.sort_values('rank_test_score')
print(f"\næŸ¥çœ‹æ’åå‰5çš„å‚æ•°ç»„åˆ:")
cols_to_display = ['rank_test_score', 'mean_test_score', 'std_test_score', 'param_C', 'param_penalty', 'param_solver']
print(cv_results_df[cols_to_display].head().to_string())

# 7. ç”¨æœ€ä½³æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

test_accuracy = accuracy_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred, average='weighted')

print("\n" + "="*40)
print("åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆæ€§èƒ½")
print("="*40)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")
print(f"æµ‹è¯•é›†F1åˆ†æ•°: {test_f1:.4f}")
print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=['è´Ÿé¢', 'æ­£é¢']))

# 8. ä¸é»˜è®¤å‚æ•°æ¨¡å‹å¯¹æ¯”ï¼ˆå±•ç¤ºè°ƒä¼˜çš„å¢ç›Šï¼‰
print("\n" + "="*40)
print("æ€§èƒ½æå‡å¯¹æ¯”")
print("="*40)
# è®­ç»ƒä¸€ä¸ªä½¿ç”¨é»˜è®¤å‚æ•°çš„æ¨¡å‹
default_model = LogisticRegression(random_state=42, max_iter=1000)
default_model.fit(X_train, y_train)
y_pred_default = default_model.predict(X_test)
default_f1 = f1_score(y_test, y_pred_default, average='weighted')

improvement = test_f1 - default_f1
print(f"é»˜è®¤å‚æ•°æ¨¡å‹F1åˆ†æ•°: {default_f1:.4f}")
print(f"è°ƒä¼˜åæœ€ä½³æ¨¡å‹F1åˆ†æ•°: {test_f1:.4f}")
print(f"F1åˆ†æ•°æå‡: {improvement:.4f} ({improvement/default_f1*100:.2f}%)")

# 9. å¯è§†åŒ–ç»“æœ
# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# å›¾1: æ··æ·†çŸ©é˜µ
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['è´Ÿé¢', 'æ­£é¢'], 
            yticklabels=['è´Ÿé¢', 'æ­£é¢'])
plt.title(f'è°ƒä¼˜åæœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ\n(æµ‹è¯•é›† F1={test_f1:.3f})', fontsize=16)
plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=14)
plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=14)
plt.tight_layout()
conf_matrix_path = './data/pandas_processed/confusion_matrix_tuned.png'
plt.savefig(conf_matrix_path, dpi=300, bbox_inches='tight')
print(f"\næ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {conf_matrix_path}")

# å›¾2: ä¸åŒCå€¼ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ˆå¯è§†åŒ–è°ƒä¼˜è¿‡ç¨‹ï¼‰
# æå–æ‰€æœ‰Cå€¼å¯¹åº”çš„ç»“æœ
c_values = []
mean_scores = []
for i, params in enumerate(cv_results_df['params']):
    if 'C' in params:
        c_values.append(params['C'])
        mean_scores.append(cv_results_df.iloc[i]['mean_test_score'])

if c_values:  # ç¡®ä¿æœ‰æ•°æ®å¯ç»˜åˆ¶
    plt.figure(figsize=(10, 6))
    # ä¸ºäº†æ¸…æ™°ï¼ŒæŒ‰Cå€¼æ’åº
    c_scores_df = pd.DataFrame({'C': c_values, 'F1_Score': mean_scores})
    c_scores_df = c_scores_df.sort_values('C')
    
    plt.plot(c_scores_df['C'], c_scores_df['F1_Score'], 'bo-', linewidth=2, markersize=8)
    plt.xscale('log')  # Cå€¼é€šå¸¸ä»¥å¯¹æ•°å°ºåº¦è§‚å¯Ÿ
    plt.xlabel('æ­£åˆ™åŒ–å¼ºåº¦ C (å¯¹æ•°å°ºåº¦)', fontsize=14)
    plt.ylabel('äº¤å‰éªŒè¯ F1 åˆ†æ•°', fontsize=14)
    plt.title('ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦ (C) ä¸‹çš„æ¨¡å‹æ€§èƒ½', fontsize=16)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    c_curve_path = './data/pandas_processed/c_parameter_curve.png'
    plt.savefig(c_curve_path, dpi=300, bbox_inches='tight')
    print(f"Cå‚æ•°æ€§èƒ½æ›²çº¿å·²ä¿å­˜è‡³: {c_curve_path}")

# 10. ä¿å­˜æœ€ä½³æ¨¡å‹å’Œç›¸å…³å…ƒæ•°æ®
print("\n" + "="*40)
print("ä¿å­˜è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹")
print("="*40)

# åŠ è½½ç‰¹å¾å·¥ç¨‹é˜¶æ®µçš„å‘é‡åŒ–å™¨
with open(os.path.join(data_dir, 'tfidf_vectorizer.pkl'), 'rb') as f:
    vectorizer = pickle.load(f)

# ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹ï¼ˆä»¥è·å¾—æœ€å¥½çš„æ³›åŒ–æ€§èƒ½ï¼‰
print("ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
final_model = LogisticRegression(**best_params, random_state=42, max_iter=2000)
final_model.fit(X, y)  # è¿™æ¬¡ä½¿ç”¨å…¨éƒ¨æ•°æ®

# æ„å»ºä¿å­˜å¯¹è±¡
model_save_path = './data/pandas_processed/tuned_best_model.pkl'
save_obj = {
    'model': final_model,
    'vectorizer': vectorizer,
    'best_params': best_params,
    'test_accuracy': test_accuracy,
    'test_f1': test_f1,
    'feature_dimension': X.shape[1],
    'training_samples': X.shape[0]
}

with open(model_save_path, 'wb') as f:
    pickle.dump(save_obj, f)

print(f"âœ… è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
print(f"   åŒ…å«: æœ€ç»ˆæ¨¡å‹ + å‘é‡åŒ–å™¨ + æœ€ä½³å‚æ•° + æ€§èƒ½æŒ‡æ ‡")

# ä¿å­˜è°ƒä¼˜æŠ¥å‘Š
report_path = './data/pandas_processed/hyperparameter_tuning_report.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ - è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š\n")
    f.write("="*50 + "\n\n")
    f.write(f"è°ƒä¼˜æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æ•°æ®è§„æ¨¡: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾\n")
    f.write(f"äº¤å‰éªŒè¯æŠ˜æ•°: 5\n")
    f.write(f"è¯„ä¼°æŒ‡æ ‡: F1_weighted\n\n")
    f.write("æœ€ä½³è¶…å‚æ•°:\n")
    for key, value in best_params.items():
        f.write(f"  {key}: {value}\n")
    f.write(f"\næœ€ä½³äº¤å‰éªŒè¯F1åˆ†æ•°: {grid_search.best_score_:.4f}\n")
    f.write(f"ç‹¬ç«‹æµ‹è¯•é›†F1åˆ†æ•°: {test_f1:.4f}\n")
    f.write(f"ç‹¬ç«‹æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}\n\n")
    f.write("æ€§èƒ½å¯¹æ¯”:\n")
    f.write(f"  é»˜è®¤å‚æ•°æ¨¡å‹F1åˆ†æ•°: {default_f1:.4f}\n")
    f.write(f"  è°ƒä¼˜åæ¨¡å‹F1åˆ†æ•°: {test_f1:.4f}\n")
    f.write(f"  ç»å¯¹æå‡: {improvement:.4f}\n")
    f.write(f"  ç›¸å¯¹æå‡: {improvement/default_f1*100:.2f}%\n")

print(f"ğŸ“Š è¯¦ç»†è°ƒä¼˜æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
print("\n" + "="*60)
print("è¶…å‚æ•°è°ƒä¼˜å®Œæˆï¼")
print("="*60)