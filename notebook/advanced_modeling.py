# advanced_modeling.py - å®Œæ•´ç‰ˆ
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
import scipy.sparse
import pickle
import os
import time
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ - è¿›é˜¶æ¨¡å‹æµ‹è¯• (éšæœºæ£®æ— & XGBoost)")
print("="*60)

# 1. åŠ è½½ç‰¹å¾å·¥ç¨‹é˜¶æ®µä¿å­˜çš„æ•°æ®
data_dir = './data/pandas_processed'
X = scipy.sparse.load_npz(os.path.join(data_dir, 'X_tfidf_features.npz'))
y = pd.read_pickle(os.path.join(data_dir, 'y_labels.pkl'))

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{y.value_counts()}")

# 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (ä¸ä¹‹å‰ä¸€è‡´)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"\nè®­ç»ƒé›†: {X_train.shape[0]} æ¡æ ·æœ¬")
print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ¡æ ·æœ¬")

# 3. å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ (XGBoostå¯¹ç¨€ç–çŸ©é˜µæ”¯æŒæœ‰é™ï¼Œå¯èƒ½éœ€è¦è½¬æ¢)
print("\næ³¨æ„: XGBoostå¯¹ç¨€ç–çŸ©é˜µæ”¯æŒæœ‰é™ï¼Œæ­£åœ¨è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µ...")
X_train_dense = X_train.toarray()
X_test_dense = X_test.toarray()
print(f"å¯†é›†çŸ©é˜µå½¢çŠ¶: {X_train_dense.shape}")

# 4. æµ‹è¯•éšæœºæ£®æ—æ¨¡å‹ (å¯ä»¥ä½¿ç”¨ç¨€ç–çŸ©é˜µ)
print("\n" + "-"*50)
print("1. éšæœºæ£®æ— (Random Forest)")
print("-"*50)

rf_model = RandomForestClassifier(
    n_estimators=100,        # æ ‘çš„æ•°é‡ï¼Œå¯è°ƒæ•´
    max_depth=None,          # æ ‘çš„æœ€å¤§æ·±åº¦
    min_samples_split=2,     # å†…éƒ¨èŠ‚ç‚¹å†åˆ’åˆ†æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=1,      # å¶å­èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
    random_state=42,
    n_jobs=-1                # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
)

# ä½¿ç”¨3æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
print("æ­£åœ¨è¿›è¡Œ3æŠ˜äº¤å‰éªŒè¯...")
rf_cv_scores = cross_val_score(rf_model, X_train, y_train, 
                               cv=3, scoring='f1_weighted', n_jobs=-1)
print(f"äº¤å‰éªŒè¯F1åˆ†æ•°: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std():.4f})")

# åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒéšæœºæ£®æ—...")
rf_start = time.time()
rf_model.fit(X_train, y_train)
rf_train_time = time.time() - rf_start
print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {rf_train_time:.2f}ç§’")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')

print(f"\næµ‹è¯•é›†æ€§èƒ½:")
print(f"å‡†ç¡®ç‡: {rf_accuracy:.4f}")
print(f"F1åˆ†æ•°: {rf_f1:.4f}")

# 5. æµ‹è¯•XGBoostæ¨¡å‹ (éœ€è¦ä½¿ç”¨å¯†é›†çŸ©é˜µ)
print("\n" + "-"*50)
print("2. XGBoost")
print("-"*50)

xgb_model = XGBClassifier(
    n_estimators=100,           # æ ‘çš„æ•°é‡
    max_depth=6,                # æ ‘çš„æœ€å¤§æ·±åº¦
    learning_rate=0.1,          # å­¦ä¹ ç‡
    objective='binary:logistic', # äºŒåˆ†ç±»é—®é¢˜
    use_label_encoder=False,    # é¿å…è­¦å‘Š
    eval_metric='logloss',      # è¯„ä¼°æŒ‡æ ‡
    random_state=42,
    n_jobs=-1                   # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
)

# ä½¿ç”¨3æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
print("æ­£åœ¨è¿›è¡Œ3æŠ˜äº¤å‰éªŒè¯...")
xgb_cv_scores = cross_val_score(xgb_model, X_train_dense, y_train, 
                                cv=3, scoring='f1_weighted', n_jobs=-1)
print(f"äº¤å‰éªŒè¯F1åˆ†æ•°: {xgb_cv_scores.mean():.4f} (+/- {xgb_cv_scores.std():.4f})")

# åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
print("åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒXGBoost...")
xgb_start = time.time()
xgb_model.fit(X_train_dense, y_train)
xgb_train_time = time.time() - xgb_start
print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {xgb_train_time:.2f}ç§’")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
y_pred_xgb = xgb_model.predict(X_test_dense)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
xgb_f1 = f1_score(y_test, y_pred_xgb, average='weighted')

print(f"\næµ‹è¯•é›†æ€§èƒ½:")
print(f"å‡†ç¡®ç‡: {xgb_accuracy:.4f}")
print(f"F1åˆ†æ•°: {xgb_f1:.4f}")

# 6. ä¸ä¹‹å‰é€»è¾‘å›å½’çš„åŸºå‡†å¯¹æ¯”
print("\n" + "="*50)
print("æ¨¡å‹æ€§èƒ½å¯¹æ¯” (æ‰€æœ‰æ¨¡å‹)")
print("="*50)

# åŠ è½½ä¹‹å‰é€»è¾‘å›å½’çš„åŸºå‡†ç»“æœ
try:
    with open(os.path.join(data_dir, 'tuned_best_model.pkl'), 'rb') as f:
        lr_data = pickle.load(f)
    lr_f1 = lr_data.get('test_f1', 0.70)
except:
    lr_f1 = 0.7003  # ä½¿ç”¨ä½ ä¹‹å‰è¿è¡Œå¾—åˆ°çš„é»˜è®¤æ¨¡å‹F1åˆ†æ•°

comparison_data = {
    'é€»è¾‘å›å½’ (åŸºå‡†)': lr_f1,
    'éšæœºæ£®æ—': rf_f1,
    'XGBoost': xgb_f1
}

comparison_df = pd.DataFrame.from_dict(comparison_data, orient='index', columns=['F1åˆ†æ•°'])
comparison_df = comparison_df.sort_values('F1åˆ†æ•°', ascending=False)
print("\nF1åˆ†æ•°å¯¹æ¯” (è¶Šé«˜è¶Šå¥½):")
print(comparison_df.to_string())

# è®¡ç®—æå‡ç™¾åˆ†æ¯”
baseline_f1 = lr_f1
for model_name, f1_score_val in comparison_data.items():
    if model_name != 'é€»è¾‘å›å½’ (åŸºå‡†)':
        improvement = f1_score_val - baseline_f1
        percent_improvement = (improvement / baseline_f1) * 100
        print(f"\n{model_name} å¯¹æ¯”åŸºå‡†:")
        print(f"  ç»å¯¹æå‡: {improvement:.4f}")
        print(f"  ç›¸å¯¹æå‡: {percent_improvement:.2f}%")

# 7. å¯è§†åŒ–å¯¹æ¯”ç»“æœ
print("\n" + "="*50)
print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
print("="*50)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
plt.figure(figsize=(10, 6))
colors = ['#2E86AB', '#A23B72', '#F18F01']  # ä¸ºä¸‰ä¸ªæ¨¡å‹è®¾ç½®ä¸åŒé¢œè‰²
bars = plt.bar(comparison_df.index, comparison_df['F1åˆ†æ•°'], color=colors, alpha=0.8)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}', ha='center', va='bottom', fontsize=12)

plt.title('ä¸åŒæ¨¡å‹åœ¨å¾®åšæƒ…æ„Ÿåˆ†æä¸Šçš„F1åˆ†æ•°å¯¹æ¯”', fontsize=16)
plt.ylabel('F1åˆ†æ•°', fontsize=14)
plt.ylim([0.65, 0.75])  # æ ¹æ®ä½ çš„ç»“æœè°ƒæ•´Yè½´èŒƒå›´
plt.axhline(y=baseline_f1, color='r', linestyle='--', alpha=0.7, label=f'åŸºå‡†çº¿ ({baseline_f1:.3f})')
plt.legend()
plt.tight_layout()

# ä¿å­˜å›¾è¡¨
chart_path = './data/pandas_processed/advanced_model_comparison.png'
plt.savefig(chart_path, dpi=300, bbox_inches='tight')
print(f"æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {chart_path}")

# 8. ä¿å­˜æœ€ä½³è¿›é˜¶æ¨¡å‹
print("\n" + "="*50)
print("ä¿å­˜æœ€ä½³è¿›é˜¶æ¨¡å‹")
print("="*50)

# ç¡®å®šæœ€ä½³æ¨¡å‹
best_model_name = comparison_df.index[0]
print(f"æœ€ä½³æ¨¡å‹: {best_model_name} (F1åˆ†æ•°: {comparison_df.iloc[0]['F1åˆ†æ•°']:.4f})")

if best_model_name == 'éšæœºæ£®æ—':
    best_model = rf_model
    model_type = 'random_forest'
    # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
    print("ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒéšæœºæ£®æ—...")
    rf_model_full = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_model_full.fit(X, y)
    
elif best_model_name == 'XGBoost':
    best_model = xgb_model
    model_type = 'xgboost'
    # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
    print("ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒXGBoost...")
    X_dense = X.toarray()
    xgb_model_full = XGBClassifier(n_estimators=100, use_label_encoder=False, 
                                   eval_metric='logloss', random_state=42)
    xgb_model_full.fit(X_dense, y)
else:
    print("é€»è¾‘å›å½’ä»æ˜¯åŸºå‡†æ¨¡å‹ï¼Œè¯·å‚è€ƒä¹‹å‰çš„è°ƒä¼˜ç»“æœã€‚")
    best_model = None

# å¦‚æœæ‰¾åˆ°äº†æ›´å¥½çš„æ¨¡å‹ï¼Œä¿å­˜å®ƒ
if best_model_name in ['éšæœºæ£®æ—', 'XGBoost']:
    # åŠ è½½ç‰¹å¾å·¥ç¨‹é˜¶æ®µçš„å‘é‡åŒ–å™¨
    with open(os.path.join(data_dir, 'tfidf_vectorizer.pkl'), 'rb') as f:
        vectorizer = pickle.load(f)
    
    # æ„å»ºä¿å­˜å¯¹è±¡
    advanced_model_path = './data/pandas_processed/advanced_best_model.pkl'
    save_obj = {
        'model_type': model_type,
        'model': rf_model_full if best_model_name == 'éšæœºæ£®æ—' else xgb_model_full,
        'vectorizer': vectorizer,
        'performance': {
            'f1_score': float(comparison_df.iloc[0]['F1åˆ†æ•°']),
            'accuracy': float(rf_accuracy if best_model_name == 'éšæœºæ£®æ—' else xgb_accuracy)
        },
        'requires_dense': best_model_name == 'XGBoost',  # XGBoostéœ€è¦å¯†é›†çŸ©é˜µ
        'feature_dimension': X.shape[1],
        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(advanced_model_path, 'wb') as f:
        pickle.dump(save_obj, f)
    
    print(f"âœ… æœ€ä½³è¿›é˜¶æ¨¡å‹å·²ä¿å­˜è‡³: {advanced_model_path}")

# 9. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
print("\n" + "="*50)
print("ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
print("="*50)

report_path = './data/pandas_processed/advanced_modeling_report.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ - è¿›é˜¶æ¨¡å‹æµ‹è¯•æŠ¥å‘Š\n")
    f.write("="*60 + "\n\n")
    f.write(f"æµ‹è¯•æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æ•°æ®è§„æ¨¡: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾\n")
    f.write(f"è®­ç»ƒé›†/æµ‹è¯•é›†: {X_train.shape[0]}/{X_test.shape[0]} æ ·æœ¬\n\n")
    
    f.write("å„æ¨¡å‹æ€§èƒ½:\n")
    f.write("-"*40 + "\n")
    f.write(f"1. é€»è¾‘å›å½’ (åŸºå‡†): F1 = {lr_f1:.4f}\n")
    f.write(f"2. éšæœºæ£®æ—: F1 = {rf_f1:.4f}\n")
    f.write(f"   è®­ç»ƒæ—¶é—´: {rf_train_time:.2f}ç§’\n")
    f.write(f"3. XGBoost: F1 = {xgb_f1:.4f}\n")
    f.write(f"   è®­ç»ƒæ—¶é—´: {xgb_train_time:.2f}ç§’\n\n")
    
    f.write("æ€§èƒ½å¯¹æ¯”æ€»ç»“:\n")
    f.write("-"*40 + "\n")
    for idx, (model, score) in enumerate(comparison_data.items()):
        rank = idx + 1
        f.write(f"{rank}. {model}: {score:.4f}\n")
    
    f.write(f"\næœ€ä½³æ¨¡å‹: {best_model_name}\n")
    f.write(f"æœ€ä½³F1åˆ†æ•°: {comparison_df.iloc[0]['F1åˆ†æ•°']:.4f}\n")
    
    if best_model_name != 'é€»è¾‘å›å½’ (åŸºå‡†)':
        improvement = comparison_df.iloc[0]['F1åˆ†æ•°'] - baseline_f1
        percent_improvement = (improvement / baseline_f1) * 100
        f.write(f"\næ€§èƒ½æå‡:\n")
        f.write(f"  ç»å¯¹æå‡: {improvement:.4f}\n")
        f.write(f"  ç›¸å¯¹æå‡: {percent_improvement:.2f}%\n")

print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
print("\n" + "="*60)
print("è¿›é˜¶æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
print("="*60)