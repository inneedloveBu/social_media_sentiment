# streamlit_app.py
from pyspark.sql.functions import col
import streamlit as st
import pandas as pd
import pickle
import jieba
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import sys
import json
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
import numpy as np

# å¯¼å…¥ä¸­æ–‡å­—ä½“é…ç½®
sys.path.append('.')
from matplotlib_config import configure_matplotlib

# é…ç½®matplotlibä¸­æ–‡å­—ä½“
configure_matplotlib()

# å¤šè¯­è¨€æ–‡æœ¬é…ç½®
TEXTS = {
    "zh": {
        "title": "ğŸ˜Š ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æç³»ç»Ÿ",
        "project_info": "â„¹ï¸ é¡¹ç›®ä¿¡æ¯",
        "project_overview": "**é¡¹ç›®æ¦‚è¿°**\n- åŸºäºæœºå™¨å­¦ä¹ çš„ä¸­æ–‡ç¤¾äº¤åª’ä½“æ–‡æœ¬æƒ…æ„Ÿåˆ†æ\n- ä½¿ç”¨å¤šç§æ¨¡å‹åœ¨å¾®åšæ•°æ®é›†ä¸Šè®­ç»ƒ\n- æœ€ä½³F1åˆ†æ•°ï¼šçº¦ 0.703",
        "tech_stack": "**æŠ€æœ¯æ ˆ**\n- æ•°æ®å¤„ç†ï¼šPandas, NumPy, Spark\n- NLPå·¥å…·ï¼šJieba (ä¸­æ–‡åˆ†è¯)\n- ç‰¹å¾å·¥ç¨‹ï¼šTF-IDF\n- æœºå™¨å­¦ä¹ ï¼šScikit-learn, XGBoost, Spark ML\n- å¯è§†åŒ–ï¼šMatplotlib, Seaborn\n- éƒ¨ç½²å±•ç¤ºï¼šStreamlit",
        "analysis_title": "ğŸ” å®æ—¶æƒ…æ„Ÿåˆ†æ",
        "input_placeholder": "ä¾‹å¦‚ï¼šè¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå‰§æƒ…æ‰£äººå¿ƒå¼¦ï¼",
        "analyze_button": "å¼€å§‹åˆ†æ",
        "analyzing": "æ­£åœ¨åˆ†ææƒ…æ„Ÿ...",
        "result_title": "åˆ†æç»“æœ",
        "sentiment": "æƒ…æ„Ÿå€¾å‘",
        "confidence": "ç½®ä¿¡åº¦",
        "probability_dist": "æƒ…æ„Ÿé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ",
        "view_details": "æŸ¥çœ‹æ–‡æœ¬å¤„ç†è¿‡ç¨‹",
        "original_text": "**åŸå§‹æ–‡æœ¬ï¼š**",
        "cleaned_text": "**æ¸…æ´—åæ–‡æœ¬ï¼š**",
        "segmented_text": "**åˆ†è¯ç»“æœï¼š**",
        "overview_title": "ğŸ“Š é¡¹ç›®æ¦‚è§ˆä¸æ€§èƒ½",
        "performance": "æ¨¡å‹æ€§èƒ½æŒ‡æ ‡",
        "data_size": "æ•°æ®é‡",
        "model_comparison": "æ¨¡å‹å¯¹æ¯”åˆ†æ",
        "highlights_title": "âœ¨ é¡¹ç›®äº®ç‚¹",
        "highlights": "- âœ… **å®Œæ•´æµç¨‹**ï¼šä»æ•°æ®æ¸…æ´—åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨æµç¨‹å®ç°\n- âœ… **ä¸­æ–‡ä¼˜åŒ–**ï¼šé’ˆå¯¹å¾®åšæ–‡æœ¬çš„ç‰¹æ®Šæ¸…æ´—å’Œåˆ†è¯å¤„ç†\n- âœ… **æ·±å…¥åˆ†æ**ï¼šå¯¹æ¯”å¤šç§æ¨¡å‹ï¼Œå¾—å‡ºå…³é”®æŠ€æœ¯æ´è§\n- âœ… **äº¤äº’å±•ç¤º**ï¼šå®æ—¶æƒ…æ„Ÿåˆ†æï¼Œç›´è§‚å‘ˆç°ç»“æœ\n- âœ… **æŠ€æœ¯æ¢ç´¢**ï¼šå°è¯•Sparkå¤§æ•°æ®å¤„ç†æ¡†æ¶",
        "summary_title": "ğŸ“‹ æŠ€æœ¯æ€»ç»“ä¸å±•æœ›",
        "implementation": "æŠ€æœ¯å®ç°",
        "implementation_details": "1. **æ•°æ®è·å–**ï¼šä½¿ç”¨WeiboSenti100Kå…¬å¼€æ•°æ®é›†\n2. **æ•°æ®é¢„å¤„ç†**ï¼šæ–‡æœ¬æ¸…æ´—ã€ä¸­æ–‡åˆ†è¯ã€åœç”¨è¯è¿‡æ»¤\n3. **ç‰¹å¾å·¥ç¨‹**ï¼šTF-IDFæ–‡æœ¬å‘é‡åŒ–ï¼ˆ2000ç»´ç‰¹å¾ï¼‰\n4. **æ¨¡å‹è®­ç»ƒ**ï¼šé€»è¾‘å›å½’ã€éšæœºæ£®æ—ã€XGBoostå¯¹æ¯”\n5. **æ¨¡å‹è°ƒä¼˜**ï¼šç½‘æ ¼æœç´¢ä¼˜åŒ–è¶…å‚æ•°\n6. **ç»“æœè¯„ä¼°**ï¼šå‡†ç¡®ç‡ã€F1åˆ†æ•°ç­‰å¤šç»´åº¦è¯„ä¼°",
        "future_work": "æœªæ¥ä¼˜åŒ–æ–¹å‘",
        "future_details": "1. **ç‰¹å¾ä¼˜åŒ–**ï¼šå°è¯•è¯å‘é‡ã€BERTç­‰æ·±åº¦å­¦ä¹ ç‰¹å¾\n2. **æ¨¡å‹å‡çº§**ï¼šä½¿ç”¨Transformeræ¨¡å‹æå‡å‡†ç¡®ç‡\n3. **æ•°æ®æ‰©å±•**ï¼šçˆ¬å–å®æ—¶å¾®åšæ•°æ®ï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§\n4. **éƒ¨ç½²ä¼˜åŒ–**ï¼šä½¿ç”¨Dockerå®¹å™¨åŒ–ï¼Œäº‘æœåŠ¡å™¨éƒ¨ç½²\n5. **åŠŸèƒ½æ‰©å±•**ï¼šæƒ…æ„ŸåŸå› åˆ†æã€ä¸»é¢˜æŒ–æ˜ç­‰",
        "run_instructions": "ğŸƒ å¦‚ä½•è¿è¡Œæ­¤åº”ç”¨",
        "installation": "**å®‰è£…ä¾èµ–ï¼š**",
        "run_command": "**è¿è¡Œåº”ç”¨ï¼š**",
        "requirements": "**ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š**",
        "model_not_found": "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²è¿è¡Œæ¨¡å‹è®­ç»ƒè„šæœ¬ã€‚",
        "no_text_warning": "è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬ã€‚",
        "positive": "æ­£é¢ ğŸ˜Š",
        "negative": "è´Ÿé¢ ğŸ˜”",
        "chart_generating": "é¡¹ç›®æ€§èƒ½å›¾è¡¨ç”Ÿæˆä¸­...",
        "chart_failed": "å›¾è¡¨åŠ è½½å¤±è´¥"
    },
    "en": {
        "title": "ğŸ˜Š Social Media Sentiment Analysis System",
        "project_info": "â„¹ï¸ Project Information",
        "project_overview": "**Project Overview**\n- Machine learning based Chinese social media text sentiment analysis\n- Using multiple models trained on Weibo dataset\n- Best F1 Score: ~0.703",
        "tech_stack": "**Technology Stack**\n- Data Processing: Pandas, NumPy, Spark\n- NLP Tool: Jieba (Chinese word segmentation)\n- Feature Engineering: TF-IDF\n  Machine Learning: Scikit-learn, XGBoost, Spark ML\n- Visualization: Matplotlib, Seaborn\n- Deployment: Streamlit",
        "analysis_title": "ğŸ” Real-time Sentiment Analysis",
        "input_placeholder": "e.g., This movie is really great, the acting is superb, and the plot is captivating!",
        "analyze_button": "Analyze Sentiment",
        "analyzing": "Analyzing sentiment...",
        "result_title": "Analysis Results",
        "sentiment": "Sentiment",
        "confidence": "Confidence",
        "probability_dist": "Sentiment Prediction Probability Distribution",
        "view_details": "View Text Processing Details",
        "original_text": "**Original Text:**",
        "cleaned_text": "**Cleaned Text:**",
        "segmented_text": "**Segmented Text:**",
        "overview_title": "ğŸ“Š Project Overview & Performance",
        "performance": "Model Performance Metrics",
        "data_size": "Data Size",
        "model_comparison": "Model Comparison Analysis",
        "highlights_title": "âœ¨ Project Highlights",
        "highlights": "- âœ… **Complete Pipeline**: Full implementation from data cleaning to model deployment\n- âœ… **Chinese Optimization**: Specialized cleaning and segmentation for Weibo text\n- âœ… **In-depth Analysis**: Comparison of multiple models with key insights\n- âœ… **Interactive Display**: Real-time sentiment analysis with intuitive results\n- âœ… **Technical Exploration**: Attempted Spark big data processing (environment issues documented)",
        "summary_title": "ğŸ“‹ Technical Summary & Future Work",
        "implementation": "Technical Implementation",
        "implementation_details": "1. **Data Acquisition**: WeiboSenti100K public dataset\n2. **Data Preprocessing**: Text cleaning, Chinese word segmentation, stop word filtering\n3. **Feature Engineering**: TF-IDF text vectorization (2000 features)\n4. **Model Training**: Comparison of Logistic Regression, Random Forest, XGBoost\n5. **Model Tuning**: Hyperparameter optimization via grid search\n6. **Evaluation**: Multi-dimensional evaluation with accuracy, F1 score, etc.",
        "future_work": "Future Optimization Directions",
        "future_details": "1. **Feature Optimization**: Try word embeddings, BERT, and deep learning features\n2. **Model Upgrade**: Use Transformer models to improve accuracy\n3. **Data Expansion**: Crawl real-time Weibo data for diversity\n4. **Deployment Optimization**: Docker containerization and cloud server deployment\n5. **Feature Expansion**: Sentiment reason analysis, topic mining, etc.",
        "run_instructions": "ğŸƒ How to Run This Application",
        "installation": "**Install Dependencies:**",
        "run_command": "**Run Application:**",
        "requirements": "**Ensure these files exist:**",
        "model_not_found": "Model file not found. Please ensure you have run the model training script.",
        "no_text_warning": "Please enter text to analyze.",
        "positive": "Positive ğŸ˜Š",
        "negative": "Negative ğŸ˜”",
        "chart_generating": "Generating performance chart...",
        "chart_failed": "Chart loading failed"
    }
}

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title="Social Media Sentiment Analysis System",
    page_icon="ğŸ˜Š",
    layout="wide"
)

# åˆå§‹åŒ–è¯­è¨€é€‰æ‹©
if 'language' not in st.session_state:
    st.session_state.language = 'zh'

# ä¾§è¾¹æ  - è¯­è¨€é€‰æ‹©
with st.sidebar:
    # è¯­è¨€åˆ‡æ¢æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ä¸­æ–‡", use_container_width=True):
            st.session_state.language = 'zh'
            st.rerun()
    with col2:
        if st.button("English", use_container_width=True):
            st.session_state.language = 'en'
            st.rerun()
    
    # ä½¿ç”¨å½“å‰è¯­è¨€è·å–æ–‡æœ¬
    t = TEXTS[st.session_state.language]
    
    st.header(t["project_info"])
    st.markdown(t["project_overview"])
    st.markdown(t["tech_stack"])
    
    st.markdown("---")
    st.caption("Project Duration: 5 days")
    st.caption("Data Scale: 119,988 labeled Weibo posts")

# æ–‡æœ¬å¤„ç†å‡½æ•°
def clean_weibo_text(text):
    """æ¸…æ´—å¾®åšæ–‡æœ¬ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå™ªå£°"""
    if not text:
        return ""
    
    # å»URL
    text = re.sub(r'https?://\S+', '', text)
    # å»@ç”¨æˆ·
    text = re.sub(r'@[\w\u4e00-\u9fa5]+', '', text)
    # å»è¡¨æƒ…ç¬¦å· [è¡¨æƒ…]
    text = re.sub(r'\[.*?\]', '', text)
    # å»ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸è§æ ‡ç‚¹
    text = re.sub(r'[^\w\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š\"\'\s]', '', text)
    # å»å¤šä½™ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def jieba_cut(text):
    """ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯"""
    if not text:
        return ""
    # åˆ†è¯å¹¶è¿”å›ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²
    return ' '.join(jieba.cut(text))

def load_stopwords(stopwords_path='./data/cn_stopwords.txt'):
    """åŠ è½½åœç”¨è¯"""
    stopwords = set()
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            for line in f:
                stopwords.add(line.strip())
    except FileNotFoundError:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çš„åœç”¨è¯
        stopwords = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½"}
    return stopwords

def remove_stopwords(text, stopwords):
    """å»é™¤åœç”¨è¯"""
    if not text:
        return ""
    words = text.split()
    filtered_words = [word for word in words if word not in stopwords]
    return ' '.join(filtered_words)

# åŠ è½½æ¨¡å‹å’Œå‘é‡åŒ–å™¨
@st.cache_resource
def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå‘é‡åŒ–å™¨"""
    try:
        model_path = './data/pandas_processed/best_sentiment_model.pkl'
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        return model_data['model'], model_data['vectorizer']
    except FileNotFoundError:
        # å°è¯•åŠ è½½è°ƒä¼˜åçš„æ¨¡å‹
        try:
            model_path = './data/pandas_processed/tuned_best_model.pkl'
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            return model_data['model'], model_data['vectorizer']
        except:
            st.error(TEXTS[st.session_state.language]["model_not_found"])
            return None, None

model, vectorizer = load_model()

@st.cache_resource
def load_spark_model():
    from pyspark.sql import SparkSession
    from pyspark.ml import PipelineModel
    from pyspark.sql.functions import udf, col
    from pyspark.sql.types import ArrayType, StringType
    import jieba
    import re
    
    spark = SparkSession.builder \
        .appName("StreamlitApp") \
        .master("local[*]") \
        .config("spark.driver.memory", "1g") \
        .getOrCreate()
    
    # åŠ è½½æ¨¡å‹
    spark_pipeline_model = PipelineModel.load("./spark_sentiment_model")
    
    # å®šä¹‰æ¸…æ´—UDF
    def clean_text_for_spark(text):
        if not text:
            return ""
        text = re.sub(r'https?://\S+', '', text)
        text = re.sub(r'@[\w\u4e00-\u9fa5]+', '', text)
        text = re.sub(r'\[.*?\]', '', text)
        text = re.sub(r'[^\w\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š\"\'\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    clean_udf = udf(clean_text_for_spark, StringType())
    
    # å®šä¹‰åˆ†è¯UDF
    def jieba_cut_for_spark(text):
        if text and isinstance(text, str):
            return list(jieba.cut(text))
        return []
    
    segment_udf = udf(jieba_cut_for_spark, ArrayType(StringType()))
    
    # å°†UDFæ³¨å†Œåˆ°Sparkä¼šè¯
    spark.udf.register("clean_udf", clean_text_for_spark, StringType())
    spark.udf.register("segment_udf", jieba_cut_for_spark, ArrayType(StringType()))
    
    return spark, spark_pipeline_model, clean_udf, segment_udf

# è·å–çœŸå®çš„æ¨¡å‹æ€§èƒ½æ•°æ®
def get_real_model_performance():
    """ä»é…ç½®æ–‡ä»¶ä¸­è·å–çœŸå®çš„æ€§èƒ½æ•°æ®"""
    config_path = "./data/pandas_processed/model_performance_config.json"
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            models_data = config['models']
            
            performance_data = {
                'models': list(models_data.keys()),
                'accuracy': [models_data[m]['accuracy'] for m in models_data],
                'f1_scores': [models_data[m]['f1_score'] for m in models_data],
                'descriptions': [models_data[m]['description'] for m in models_data]
            }
            
            return performance_data
            
        except Exception as e:
            st.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ•°æ®
    return {
        'models': ['é€»è¾‘å›å½’', 'éšæœºæ£®æ—', 'XGBoost', 'Spark ML', 'PandasåŸºç¡€ç‰ˆ'],
        'accuracy': [0.701, 0.672, 0.614, 0.685, 0.698],
        'f1_scores': [0.703, 0.674, 0.612, 0.682, 0.700],
        'descriptions': ['ç»è¿‡è°ƒä¼˜çš„é€»è¾‘å›å½’', '100æ£µå†³ç­–æ ‘', 'æ¢¯åº¦æå‡æ ‘', 'Sparkåˆ†å¸ƒå¼æ¨¡å‹', 'åŸºç¡€é€»è¾‘å›å½’']
    }

# è·å–å½“å‰è¯­è¨€çš„æ–‡æœ¬
t = TEXTS[st.session_state.language]

# åº”ç”¨æ ‡é¢˜
st.title(t["title"])
st.markdown("---")

# ä¸»ç•Œé¢åˆ†ä¸ºä¸¤åˆ—
main_col1, main_col2 = st.columns([1, 1])

# å­˜å‚¨åˆ†æç»“æœçš„session state
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None

with main_col1:
    st.header(t["analysis_title"])
    
    # æ–‡æœ¬è¾“å…¥åŒº
    user_input = st.text_area(
        "è¯·è¾“å…¥è¦åˆ†æçš„ä¸­æ–‡æ–‡æœ¬ï¼š" if st.session_state.language == 'zh' else "Enter Chinese text to analyze:",
        height=150,
        placeholder=t["input_placeholder"],
        help="æ”¯æŒå¾®åšã€è¯„è®ºã€çŸ­æ–‡æœ¬ç­‰ä¸­æ–‡å†…å®¹" if st.session_state.language == 'zh' else "Supports Chinese content like Weibo, comments, short texts",
        key="user_input"
    )
    
    # åˆ†ææŒ‰é’®
    analyze_clicked = st.button(t["analyze_button"], type="primary", use_container_width=True)
    
    if analyze_clicked:
        if user_input.strip():
            with st.spinner(t["analyzing"]):
                # åŠ è½½åœç”¨è¯
                stopwords = load_stopwords()
                
                # Scikit-learnæ¨¡å‹é¢„æµ‹
                skl_sentiment = None
                skl_confidence = None
                skl_probability = None
                cleaned_text = clean_weibo_text(user_input)
                segmented_text = jieba_cut(cleaned_text)
                filtered_text = remove_stopwords(segmented_text, stopwords)
                
                if vectorizer and model:
                    features = vectorizer.transform([segmented_text])
                    skl_prediction = model.predict(features)[0]
                    skl_probability = model.predict_proba(features)[0]
                    skl_sentiment = t["positive"] if skl_prediction == 1 else t["negative"]
                    skl_confidence = skl_probability[1] if skl_prediction == 1 else skl_probability[0]
                else:
                    st.error("Scikit-learnæ¨¡å‹åŠ è½½å¤±è´¥ã€‚")
                
                # Sparkæ¨¡å‹é¢„æµ‹
                spark_sentiment = None
                spark_confidence = None
                spark_probability = None
                try:
                    spark_session, spark_pipeline_model, clean_udf, segment_udf = load_spark_model()
                    input_df = spark_session.createDataFrame([(user_input,)], ["text"])
                    input_df = input_df.withColumn("cleaned", clean_udf(col("text")))
                    input_df = input_df.withColumn("words", segment_udf(col("cleaned")))
                    spark_prediction_row = spark_pipeline_model.transform(input_df).collect()[0]
                    
                    # è·å–Sparkæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
                    # Spark MLçš„probabilityæ˜¯ä¸€ä¸ªDenseVector
                    spark_probability_list = spark_prediction_row.probability
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    spark_probability = np.array(spark_probability_list.toArray())
                    
                    spark_sentiment = "æ­£é¢ ğŸ˜Š" if spark_prediction_row.prediction == 1 else "è´Ÿé¢ ğŸ˜”"
                    spark_confidence = spark_probability[1] if spark_prediction_row.prediction == 1 else spark_probability[0]
                    
                except Exception as e:
                    st.error(f"Sparkæ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                
                # ä¿å­˜ç»“æœåˆ°session state
                st.session_state.analysis_results = {
                    'user_input': user_input,
                    'cleaned_text': cleaned_text,
                    'segmented_text': segmented_text,
                    'skl_sentiment': skl_sentiment,
                    'skl_confidence': skl_confidence,
                    'skl_probability': skl_probability,
                    'spark_sentiment': spark_sentiment,
                    'spark_confidence': spark_confidence,
                    'spark_probability': spark_probability
                }
        else:
            st.warning(t["no_text_warning"])

# å¦‚æœæœ‰åˆ†æç»“æœï¼Œæ˜¾ç¤ºåœ¨ä¸¤ä¸ªåˆ—ä¸­
if st.session_state.analysis_results:
    results = st.session_state.analysis_results
    
    # åœ¨å·¦æ æ˜¾ç¤ºå®æ—¶åˆ†æç»“æœ
    with main_col1:
        st.markdown(f"### {t['result_title']}")
        
        # ä½¿ç”¨ä¸¤åˆ—å¹¶æ’æ˜¾ç¤ºä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ
        result_col1, result_col2 = st.columns(2)
        
        with result_col1:
            st.subheader("ğŸ¤– Scikit-learn æ¨¡å‹")
            if results['skl_sentiment']:
                # æƒ…æ„Ÿå€¾å‘å¡ç‰‡
                sentiment_color = "ğŸŸ¢" if "æ­£é¢" in results['skl_sentiment'] else "ğŸ”´"
                st.metric(t["sentiment"], f"{sentiment_color} {results['skl_sentiment']}")
                
                # ç½®ä¿¡åº¦å¡ç‰‡
                confidence_color = "ğŸŸ¢" if results['skl_confidence'] > 0.7 else "ğŸŸ¡" if results['skl_confidence'] > 0.5 else "ğŸ”´"
                st.metric(t["confidence"], f"{confidence_color} {results['skl_confidence']:.2%}")
                
                # é¢„æµ‹æ¦‚ç‡
                if results['skl_probability'] is not None:
                    with st.expander("ğŸ“Š è¯¦ç»†æ¦‚ç‡", expanded=True):
                        negative_prob = results['skl_probability'][0]
                        positive_prob = results['skl_probability'][1]
                        
                        st.progress(positive_prob, text=f"æ­£é¢æ¦‚ç‡: {positive_prob:.2%}")
                        st.progress(negative_prob, text=f"è´Ÿé¢æ¦‚ç‡: {negative_prob:.2%}")
            else:
                st.error("Scikit-learnæ¨¡å‹é¢„æµ‹å¤±è´¥")
        
        with result_col2:
            st.subheader("ğŸš€ Spark ML æ¨¡å‹")
            if results['spark_sentiment']:
                # æƒ…æ„Ÿå€¾å‘å¡ç‰‡
                sentiment_color = "ğŸŸ¢" if "æ­£é¢" in results['spark_sentiment'] else "ğŸ”´"
                st.metric("æƒ…æ„Ÿå€¾å‘", f"{sentiment_color} {results['spark_sentiment']}")
                
                # ç½®ä¿¡åº¦å¡ç‰‡
                confidence_color = "ğŸŸ¢" if results['spark_confidence'] > 0.7 else "ğŸŸ¡" if results['spark_confidence'] > 0.5 else "ğŸ”´"
                st.metric("ç½®ä¿¡åº¦", f"{confidence_color} {results['spark_confidence']:.2%}")
                
                # é¢„æµ‹æ¦‚ç‡
                if results['spark_probability'] is not None:
                    with st.expander("ğŸ“Š è¯¦ç»†æ¦‚ç‡", expanded=True):
                        negative_prob = results['spark_probability'][0]
                        positive_prob = results['spark_probability'][1]
                        
                        st.progress(positive_prob, text=f"æ­£é¢æ¦‚ç‡: {positive_prob:.2%}")
                        st.progress(negative_prob, text=f"è´Ÿé¢æ¦‚ç‡: {negative_prob:.2%}")
            else:
                st.error("Spark MLæ¨¡å‹é¢„æµ‹å¤±è´¥")
        
        st.markdown("---")
        
        # ä¸¤ä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”å›¾è¡¨
        if results['skl_probability'] is not None and results['spark_probability'] is not None:
            st.subheader("ğŸ“Š åŒæ¨¡å‹æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”")
            
            # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            
            # Scikit-learnæ¨¡å‹æ¦‚ç‡
            sentiments_skl = ['è´Ÿé¢', 'æ­£é¢'] if st.session_state.language == 'zh' else ['Negative', 'Positive']
            colors_skl = ['#FF6B6B', '#4ECDC4']
            bars_skl = ax1.bar(sentiments_skl, results['skl_probability'], color=colors_skl, alpha=0.8)
            ax1.set_title('Scikit-learn æ¨¡å‹')
            ax1.set_ylabel('æ¦‚ç‡')
            ax1.set_ylim([0, 1.1])
            
            for bar, prob in zip(bars_skl, results['skl_probability']):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2%}', ha='center', va='bottom', fontsize=10)
            
            # Spark MLæ¨¡å‹æ¦‚ç‡
            sentiments_spark = ['è´Ÿé¢', 'æ­£é¢'] if st.session_state.language == 'zh' else ['Negative', 'Positive']
            colors_spark = ['#FF6B6B', '#4ECDC4']
            bars_spark = ax2.bar(sentiments_spark, results['spark_probability'], color=colors_spark, alpha=0.8)
            ax2.set_title('Spark ML æ¨¡å‹')
            ax2.set_ylabel('æ¦‚ç‡')
            ax2.set_ylim([0, 1.1])
            
            for bar, prob in zip(bars_spark, results['spark_probability']):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{prob:.2%}', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            st.pyplot(fig)
            
            # æ·»åŠ å¯¹æ¯”åˆ†æ
            if results['skl_sentiment'] == results['spark_sentiment']:
                st.success("âœ… ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœä¸€è‡´ï¼")
            else:
                st.warning("âš ï¸ ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœä¸ä¸€è‡´ï¼Œå»ºè®®ç»“åˆä¸Šä¸‹æ–‡åˆ¤æ–­ã€‚")
        
        # æ–‡æœ¬å¤„ç†è¯¦æƒ…
        with st.expander(t["view_details"], expanded=False):
            st.write(t["original_text"])
            st.info(results['user_input'])
            
            st.write(t["cleaned_text"])
            st.success(results['cleaned_text'])
            
            st.write(t["segmented_text"])
            st.code(results['segmented_text'])

# å³æ ï¼šé¡¹ç›®æ¦‚è§ˆå’Œæ€§èƒ½å¯¹æ¯”
with main_col2:
    st.header(t["overview_title"])
    
    # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½
    st.subheader(t["performance"])
    
    # è·å–çœŸå®æ€§èƒ½æ•°æ®
    performance_data = get_real_model_performance()
    
    # æ‰¾åˆ°æœ€ä½³F1åˆ†æ•°å’Œå‡†ç¡®ç‡
    best_f1_index = performance_data['f1_scores'].index(max(performance_data['f1_scores']))
    best_model = performance_data['models'][best_f1_index]
    best_f1 = max(performance_data['f1_scores'])
    
    best_acc_index = performance_data['accuracy'].index(max(performance_data['accuracy']))
    best_acc_model = performance_data['models'][best_acc_index]
    best_acc = max(performance_data['accuracy'])
    
    # åˆ›å»ºæŒ‡æ ‡å¡ç‰‡
    metric_col1, metric_col2, metric_col3 = st.columns(3)
    
    with metric_col1:
        st.metric("æœ€ä½³F1åˆ†æ•°", f"{best_f1:.3f}", f"{best_model}")
    
    with metric_col2:
        st.metric("æœ€ä½³å‡†ç¡®ç‡", f"{best_acc:.3f}", f"{best_acc_model}")
    
    with metric_col3:
        st.metric(t["data_size"], "119,988", "å¾®åšæ•°æ®")
    
    # æ¨¡å‹å¯¹æ¯”åˆ†æå›¾è¡¨
    st.subheader(t["model_comparison"])
    
    try:
        # é‡æ–°é…ç½®ä¸­æ–‡å­—ä½“
        configure_matplotlib()
        
        # æ ¹æ®è¯­è¨€è®¾ç½®æ¨¡å‹åç§°
        if st.session_state.language == 'zh':
            models = performance_data['models']
        else:
            # è‹±æ–‡åç§°æ˜ å°„
            model_mapping = {
                'é€»è¾‘å›å½’': 'Logistic Regression',
                'éšæœºæ£®æ—': 'Random Forest',
                'XGBoost': 'XGBoost',
                'Spark ML': 'Spark ML',
                'PandasåŸºç¡€ç‰ˆ': 'Pandas Baseline'
            }
            models = [model_mapping.get(m, m) for m in performance_data['models']]
        
        # å‡†å¤‡æ•°æ®
        accuracy_scores = performance_data['accuracy']
        f1_scores = performance_data['f1_scores']
        
        # é¢œè‰²è®¾ç½®
        colors = ['#4ECDC4', '#FF6B6B', '#FFE66D', '#95E1D3', '#FF9A8B']
        
        # åˆ›å»ºä¸¤ä¸ªå­å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # å‡†ç¡®ç‡å›¾è¡¨
        bars1 = ax1.bar(models, accuracy_scores, color=colors, alpha=0.8)
        ax1.set_ylabel('å‡†ç¡®ç‡' if st.session_state.language == 'zh' else 'Accuracy')
        ax1.set_ylim([0.5, 0.75])
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”' if st.session_state.language == 'zh' else 'Model Accuracy Comparison')
        ax1.grid(True, alpha=0.3, axis='y')
        ax1.tick_params(axis='x', rotation=15)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars1, accuracy_scores):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
        
        # F1åˆ†æ•°å›¾è¡¨
        bars2 = ax2.bar(models, f1_scores, color=colors, alpha=0.8)
        ax2.set_ylabel('F1åˆ†æ•°' if st.session_state.language == 'zh' else 'F1 Score')
        ax2.set_ylim([0.5, 0.75])
        ax2.set_title('æ¨¡å‹F1åˆ†æ•°å¯¹æ¯”' if st.session_state.language == 'zh' else 'Model F1 Score Comparison')
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.tick_params(axis='x', rotation=15)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, f1 in zip(bars2, f1_scores):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{f1:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # æ·»åŠ è¯¦ç»†æ•°æ®è¡¨æ ¼
        with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ€§èƒ½æ•°æ®", expanded=False):
            if st.session_state.language == 'zh':
                df_performance = pd.DataFrame({
                    'æ¨¡å‹': performance_data['models'],
                    'å‡†ç¡®ç‡': [f"{acc:.4f}" for acc in accuracy_scores],
                    'F1åˆ†æ•°': [f"{f1:.4f}" for f1 in f1_scores],
                    'æ’å': [sorted(f1_scores, reverse=True).index(f1) + 1 for f1 in f1_scores]
                })
            else:
                df_performance = pd.DataFrame({
                    'Model': models,
                    'Accuracy': [f"{acc:.4f}" for acc in accuracy_scores],
                    'F1 Score': [f"{f1:.4f}" for f1 in f1_scores],
                    'Rank': [sorted(f1_scores, reverse=True).index(f1) + 1 for f1 in f1_scores]
                })
            df_performance = df_performance.sort_values('æ’å' if st.session_state.language == 'zh' else 'Rank')
            st.dataframe(df_performance, use_container_width=True, hide_index=True)
        
        # æ·»åŠ åˆ†ææ€»ç»“
        st.info(f"""
        **åˆ†ææ€»ç»“**: 
        - åœ¨æµ‹è¯•çš„{len(models)}ä¸ªæ¨¡å‹ä¸­ï¼Œ**{best_model}** è¡¨ç°æœ€ä½³
        - æœ€ä½³F1åˆ†æ•°ï¼š**{best_f1:.3f}**ï¼Œæœ€ä½³å‡†ç¡®ç‡ï¼š**{best_acc:.3f}**
        - é€»è¾‘å›å½’æ¨¡å‹åœ¨æ€§èƒ½å’Œè®­ç»ƒé€Ÿåº¦ä¸Šå–å¾—äº†æœ€ä½³å¹³è¡¡
        - Spark MLæ¨¡å‹å±•ç¤ºäº†åˆ†å¸ƒå¼è®¡ç®—åœ¨æƒ…æ„Ÿåˆ†æä¸­çš„æ½œåŠ›
        """)
        
    except Exception as e:
        st.warning(f"{t['chart_failed']}: {str(e)}")

# åº•éƒ¨çš„é¡¹ç›®æ€»ç»“
st.markdown("---")
st.header(t["summary_title"])

summary_col1, summary_col2, summary_col3 = st.columns(3)

with summary_col1:
    st.subheader(t["highlights_title"])
    st.markdown(t["highlights"])

with summary_col2:
    st.subheader(t["implementation"])
    st.markdown(t["implementation_details"])

with summary_col3:
    st.subheader(t["future_work"])
    st.markdown(t["future_details"])

# è¿è¡Œè¯´æ˜
with st.expander(t["run_instructions"], expanded=False):
    st.markdown(f"""
    **{t['installation']}**
    ```bash
    pip install streamlit pandas scikit-learn jieba matplotlib seaborn pyspark xgboost
    ```
    
    **{t['run_command']}**
    ```bash
    streamlit run streamlit_app.py
    ```
    
    **{t['requirements']}**
    - `./data/pandas_processed/best_sentiment_model.pkl` (è®­ç»ƒå¥½çš„æ¨¡å‹)
    - `./spark_sentiment_model` (Sparkæ¨¡å‹ç›®å½•)
    - `./data/cn_stopwords.txt` (åœç”¨è¯æ–‡ä»¶ï¼Œå¯é€‰)
    """)

st.markdown("---")
if st.session_state.language == 'zh':
    st.caption("Â© 2025 ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æé¡¹ç›® | åŸºäºPythonçš„æ•°æ®ç§‘å­¦ä½œå“é›†")
else:
    st.caption("Â© 2025 Social Media Sentiment Analysis Project | Python Data Science Portfolio")