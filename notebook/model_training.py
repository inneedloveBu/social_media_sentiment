# model_training.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc
import scipy.sparse
import pickle
import os
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("å¼€å§‹ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")
print("="*60)

# 1. åŠ è½½ç‰¹å¾å·¥ç¨‹é˜¶æ®µä¿å­˜çš„æ•°æ®
data_dir = './data/pandas_processed'
X = scipy.sparse.load_npz(os.path.join(data_dir, 'X_tfidf_features.npz'))
y = pd.read_pickle(os.path.join(data_dir, 'y_labels.pkl'))

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{y.value_counts()}")

# 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80%è®­ç»ƒï¼Œ20%æµ‹è¯•)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"\nè®­ç»ƒé›†æ ·æœ¬æ•°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test.shape[0]}")

# 3. åˆå§‹åŒ–è¦æ¯”è¾ƒçš„æ¨¡å‹
models = {
    'é€»è¾‘å›å½’ (Logistic Regression)': LogisticRegression(random_state=42, max_iter=1000),
    'éšæœºæ£®æ— (Random Forest)': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'æ”¯æŒå‘é‡æœº (Linear SVM)': LinearSVC(random_state=42, max_iter=1000)
}

# 4. è®­ç»ƒã€é¢„æµ‹å¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹
results = {}
predictions = {}
feature_importances = {}  # ç”¨äºå­˜å‚¨æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæœ‰ï¼‰

for name, model in models.items():
    print(f"\n{'='*40}")
    print(f"è®­ç»ƒæ¨¡å‹: {name}")
    print(f"{'='*40}")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
    y_pred = model.predict(X_test)
    predictions[name] = (y_test, y_pred)  # ä¿å­˜çœŸå®å€¼å’Œé¢„æµ‹å€¼
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    results[name] = {
        'å‡†ç¡®ç‡ (Accuracy)': accuracy,
        'ç²¾ç¡®ç‡ (Precision)': precision,
        'å¬å›ç‡ (Recall)': recall,
        'F1åˆ†æ•° (F1-Score)': f1
    }
    
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"å¬å›ç‡: {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1:.4f}")
    
    # å°è¯•è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä»…é€‚ç”¨äºæœ‰è¯¥å±æ€§çš„æ¨¡å‹ï¼Œå¦‚éšæœºæ£®æ—ï¼‰
    if hasattr(model, 'coef_'):
        feature_importances[name] = np.abs(model.coef_[0])
    elif hasattr(model, 'feature_importances_'):
        feature_importances[name] = model.feature_importances_

# 5. å¯¹æ¯”æ‰€æœ‰æ¨¡å‹ç»“æœ
print("\n" + "="*60)
print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ€»ç»“")
print("="*60)

results_df = pd.DataFrame(results).T.round(4)
print(results_df)

# 6. å¯è§†åŒ–ç»“æœ
# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
sns.set_style("whitegrid")

# å›¾1: æ¨¡å‹æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.ravel()

metrics_to_plot = ['å‡†ç¡®ç‡ (Accuracy)', 'ç²¾ç¡®ç‡ (Precision)', 'å¬å›ç‡ (Recall)', 'F1åˆ†æ•° (F1-Score)']
for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx]
    results_df[metric].plot(kind='bar', ax=ax, color=sns.color_palette("husl", len(models)))
    ax.set_title(f'{metric} å¯¹æ¯”', fontsize=14)
    ax.set_ylabel(metric, fontsize=12)
    ax.set_ylim([0.8, 1.0])  # æƒ…æ„Ÿåˆ†æä»»åŠ¡æŒ‡æ ‡é€šå¸¸è¾ƒé«˜ï¼Œèšç„¦äºé«˜åˆ†æ®µ
    ax.tick_params(axis='x', rotation=45)
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, v in enumerate(results_df[metric]):
        ax.text(i, v + 0.005, f'{v:.3f}', ha='center', fontsize=10)

plt.tight_layout()
plt.savefig('./data/pandas_processed/model_comparison.png', dpi=300, bbox_inches='tight')
print(f"\næ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: ./data/pandas_processed/model_comparison.png")

# å›¾2: æœ€ä½³æ¨¡å‹çš„æ··æ·†çŸ©é˜µ (é€‰æ‹©F1åˆ†æ•°æœ€é«˜çš„æ¨¡å‹)
best_model_name = results_df['F1åˆ†æ•° (F1-Score)'].idxmax()
y_test_best, y_pred_best = predictions[best_model_name]

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test_best, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['è´Ÿé¢', 'æ­£é¢'], yticklabels=['è´Ÿé¢', 'æ­£é¢'])
plt.title(f'æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ: {best_model_name}', fontsize=16)
plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=14)
plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=14)
plt.tight_layout()
plt.savefig('./data/pandas_processed/confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')
print(f"æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: ./data/pandas_processed/confusion_matrix_best_model.png")

# 7. ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ‰€æœ‰ç»“æœ
print("\n" + "="*60)
print("ä¿å­˜æ¨¡å‹ä¸ç»“æœ")
print("="*60)

# åŠ è½½ç‰¹å¾å·¥ç¨‹é˜¶æ®µä¿å­˜çš„å‘é‡åŒ–å™¨ï¼Œä¸æœ€ä½³æ¨¡å‹ä¸€èµ·ä¿å­˜
with open(os.path.join(data_dir, 'tfidf_vectorizer.pkl'), 'rb') as f:
    vectorizer = pickle.load(f)

best_model = models[best_model_name]
best_model.fit(X, y)  # ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒä¸€æ¬¡ï¼Œä»¥è·å¾—æœ€å¥½çš„æ³›åŒ–èƒ½åŠ›

model_save_path = './data/pandas_processed/best_sentiment_model.pkl'
with open(model_save_path, 'wb') as f:
    pickle.dump({
        'model': best_model,
        'vectorizer': vectorizer,
        'model_name': best_model_name,
        'performance': results_df.loc[best_model_name].to_dict()
    }, f)

print(f"âœ… æœ€ä½³æ¨¡å‹ ({best_model_name}) å·²ä¿å­˜è‡³: {model_save_path}")
print(f"   åŒ…å«: è®­ç»ƒå¥½çš„æ¨¡å‹ + TF-IDFå‘é‡åŒ–å™¨ + æ€§èƒ½æŒ‡æ ‡")

# ä¿å­˜è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
report_save_path = './data/pandas_processed/detailed_classification_report.txt'
with open(report_save_path, 'w', encoding='utf-8') as f:
    f.write("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æé¡¹ç›® - è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n")
    f.write("="*50 + "\n\n")
    f.write(f"æ•°æ®è§„æ¨¡: {X.shape[0]} æ¡æ ·æœ¬ï¼Œ {X.shape[1]} ä¸ªç‰¹å¾\n")
    f.write(f"è®­ç»ƒ/æµ‹è¯•æ¯”ä¾‹: 80%/20%\n")
    f.write(f"æœ€ä½³æ¨¡å‹: {best_model_name}\n\n")
    f.write("å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\n")
    f.write(results_df.to_string() + "\n\n")
    f.write(f"\næœ€ä½³æ¨¡å‹ ({best_model_name}) çš„è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\n")
    f.write(classification_report(y_test_best, y_pred_best, target_names=['è´Ÿé¢', 'æ­£é¢']))

print(f"ğŸ“Š è¯¦ç»†è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_save_path}")

print("\n" + "="*60)
print("æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°é˜¶æ®µå…¨éƒ¨å®Œæˆï¼")
print("="*60)
print("\nä¸‹ä¸€æ­¥å»ºè®®:")
print("1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶ï¼Œäº†è§£æ¨¡å‹æ€§èƒ½ã€‚")
print("2. å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„é¢„æµ‹è„šæœ¬ï¼Œè¾“å…¥æ–°æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ã€‚")
print("3. è€ƒè™‘ä½¿ç”¨Streamlitæ„å»ºä¸€ä¸ªäº¤äº’å¼Webåº”ç”¨è¿›è¡Œå±•ç¤ºã€‚")