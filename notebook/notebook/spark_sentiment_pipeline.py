# spark_sentiment_pipeline.py - ä¿®å¤ç‰ˆ
import os
import sys

# ====== 1. åº”ç”¨Windowsä¿®å¤è¡¥ä¸ ======
# å¯¼å…¥ä¿®å¤è¡¥ä¸
import spark_windows_patch  # ç¡®ä¿è¿™ä¸ªæ–‡ä»¶åœ¨ç›¸åŒç›®å½•

print("="*60)
print("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ - Spark ML å…¨æµç¨‹å®ç° (ä¿®å¤ç‰ˆ)")
print("="*60)

# ====== 2. å»¶è¿Ÿå¯¼å…¥Sparkç›¸å…³æ¨¡å— ======
# å…ˆå¯¼å…¥åŸºç¡€åº“
import pandas as pd
import jieba

# ====== 3. åˆå§‹åŒ–SparkSession ======
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import udf, col, regexp_replace, when
    from pyspark.sql.types import ArrayType, StringType, FloatType
    from pyspark.ml import Pipeline
    from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover
    from pyspark.ml.classification import LogisticRegression
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
    
    print("âœ… PySparkæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ PySparkå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install pyspark")
    sys.exit(1)

# ====== 4. åˆ›å»ºSparkSession ======
def create_spark_session():
    """åˆ›å»ºå¹¶é…ç½®SparkSession"""
    try:
        spark = SparkSession.builder \
            .appName("Weibo_Sentiment_Analysis_Spark") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "2g") \
            .config("spark.executor.cores", "2") \
            .config("spark.driver.cores", "2") \
            .config("spark.python.worker.timeout", "300") \
            .config("spark.python.worker.reuse", "false") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.sql.execution.arrow.pyspark.fallback.enabled", "true") \
            .config("spark.sql.shuffle.partitions", "50") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.port", "9999") \
            .master("local[*]") \
            .getOrCreate()
        
        print("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸï¼")
        print(f"Sparkç‰ˆæœ¬: {spark.version}")
        return spark
    except Exception as e:
        print(f"âŒ Sparkä¼šè¯åˆ›å»ºå¤±è´¥: {e}")
        return None

# ====== 5. ä¸»ç¨‹åºé€»è¾‘ ======
def main():
    # åˆ›å»ºSparkä¼šè¯
    spark = create_spark_session()
    if spark is None:
        return
    
    try:
        # ====== åŠ è½½æ•°æ® ======
        data_path = "./data/raw_data_backup.csv"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            print("åˆ›å»ºç¤ºä¾‹æ•°æ®...")
            sample_data = pd.DataFrame({
                'review': [
                    'è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆå¥½çœ‹ï¼æ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå‰§æƒ…ä¹Ÿå¾ˆå¸å¼•äººï¼',
                    'å¤ªå¤±æœ›äº†ï¼Œæµªè´¹æ—¶é—´å’Œé‡‘é’±ï¼Œå®Œå…¨ä¸å€¼å¾—çœ‹ã€‚',
                    'ä¸­è§„ä¸­çŸ©å§ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«å‡ºå½©çš„åœ°æ–¹ã€‚',
                    'å¼ºçƒˆæ¨èï¼è¿™æ˜¯æˆ‘ä»Šå¹´çœ‹è¿‡æœ€å¥½çš„ç”µå½±ï¼',
                    'éå¸¸ç³Ÿç³•çš„ä½“éªŒï¼Œå¯¼æ¼”åˆ°åº•åœ¨æƒ³ä»€ä¹ˆï¼Ÿ'
                ],
                'label': [1, 0, 0, 1, 0]
            })
            sample_data.to_csv(data_path, index=False, encoding='utf-8-sig')
            print(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ•°æ®: {data_path}")
        
        # è¯»å–æ•°æ®
        print(f"ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®: {data_path}")
        df_raw = spark.read.csv(data_path, header=True, inferSchema=True, encoding="utf-8")
        print(f"åŸå§‹æ•°æ®è¡Œæ•°ï¼š{df_raw.count()}")
        df_raw.show(5, truncate=50)
        
        # ====== æ•°æ®æ¸…æ´— ======
        print("\nğŸ”§ å¼€å§‹æ•°æ®æ¸…æ´—...")
        df_cleaned = df_raw.withColumn(
            "cleaned_review",
            regexp_replace(
                regexp_replace(
                    regexp_replace(
                        col("review"),
                        r'https?://\S+', ''  # å»é™¤URL
                    ),
                    r'@[\w\u4e00-\u9fa5]+', ''  # å»é™¤@ç”¨æˆ·
                ),
                r'\[.*?\]', ''  # å»é™¤[è¡¨æƒ…]
            )
        )
        
        df_cleaned = df_cleaned.withColumn(
            "cleaned_review",
            regexp_replace(col("cleaned_review"), r'[^\w\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š\"\'\s]', '')
        ).withColumn(
            "cleaned_review",
            regexp_replace(col("cleaned_review"), r'\s+', ' ')
        )
        
        print("æ¸…æ´—åæ ·æœ¬ç¤ºä¾‹ï¼š")
        df_cleaned.select("review", "cleaned_review").show(3, truncate=50)
        
        # ====== ä¸­æ–‡åˆ†è¯ ======
        print("\nğŸ”ª å¼€å§‹ä¸­æ–‡åˆ†è¯...")
        
        # å®šä¹‰æ™®é€šUDFï¼ˆæ›´ç¨³å®šï¼‰
        from pyspark.sql.functions import udf
        from pyspark.sql.types import ArrayType, StringType
        
        def jieba_segment(text):
            """ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯"""
            if not text or not isinstance(text, str):
                return []
            try:
                return list(jieba.cut(text.strip()))
            except Exception:
                return []
        
        # æ³¨å†ŒUDF
        segment_udf = udf(jieba_segment, ArrayType(StringType()))
        
        # åº”ç”¨UDF
        df_segmented = df_cleaned.withColumn("words", segment_udf(col("cleaned_review")))
        
        # è¿‡æ»¤ç©ºåˆ†è¯ç»“æœ
        from pyspark.sql.functions import size
        df_segmented = df_segmented.filter(size(col("words")) > 0)
        
        print(f"åˆ†è¯åæœ‰æ•ˆæ•°æ®è¡Œæ•°ï¼š{df_segmented.count()}")
        df_segmented.select("cleaned_review", "words").show(3, truncate=False)
        
        # ====== åŠ è½½åœç”¨è¯ ======
        stopwords_path = "./data/cn_stopwords.txt"
        stop_words_list = []
        
        if os.path.exists(stopwords_path):
            with open(stopwords_path, 'r', encoding='utf-8') as f:
                stop_words_list = [line.strip() for line in f if line.strip()]
            print(f"ğŸ“– å·²ä»æ–‡ä»¶åŠ è½½ {len(stop_words_list)} ä¸ªåœç”¨è¯")
        else:
            # åŸºç¡€åœç”¨è¯è¡¨
            stop_words_list = ["çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", 
                             "ä¸", "äºº", "éƒ½", "ä¹Ÿ", "è€Œ", "åŠ", "ä¸", "ç€", 
                             "æˆ–", "ä¸ª", "æ²¡æœ‰", "è¿™", "é‚£", "å°±", "ä¹Ÿ"]
            print(f"âš ï¸  æœªæ‰¾åˆ°å¤–éƒ¨åœç”¨è¯æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½® {len(stop_words_list)} ä¸ªåœç”¨è¯")
        
        # ====== ç‰¹å¾å·¥ç¨‹ ======
        print("\nâš™ï¸  å¼€å§‹ç‰¹å¾å·¥ç¨‹ï¼ˆTF-IDFï¼‰...")
        
        # ç§»é™¤åœç”¨è¯
        stopwords_remover = StopWordsRemover(
            inputCol="words",
            outputCol="filtered_words",
            stopWords=stop_words_list
        )
        
        # TF-IDF
        hashing_tf = HashingTF(
            inputCol="filtered_words",
            outputCol="raw_features",
            numFeatures=1000  # é™ä½ç‰¹å¾ç»´åº¦ä»¥æé«˜é€Ÿåº¦
        )
        
        idf = IDF(
            inputCol="raw_features",
            outputCol="features",
            minDocFreq=1
        )
        
        # ====== æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹ ======
        print("\nğŸ¤– æ„å»ºæœºå™¨å­¦ä¹ Pipelineå¹¶è®­ç»ƒ...")
        
        # é€»è¾‘å›å½’åˆ†ç±»å™¨
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="label",
            maxIter=50,  # å‡å°‘è¿­ä»£æ¬¡æ•°
            regParam=0.1,
            elasticNetParam=0
        )
        
        # æ„å»ºPipeline
        pipeline = Pipeline(stages=[
            stopwords_remover,
            hashing_tf,
            idf,
            lr
        ])
        
        # åˆ’åˆ†æ•°æ®é›†
        train_df, test_df = df_segmented.randomSplit([0.7, 0.3], seed=42)
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {train_df.count()}")
        print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {test_df.count()}")
        
        # è®­ç»ƒæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        pipeline_model = pipeline.fit(train_df)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # ====== æ¨¡å‹è¯„ä¼° ======
        print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
        predictions = pipeline_model.transform(test_df)
        
        evaluator_f1 = MulticlassClassificationEvaluator(
            labelCol="label",
            predictionCol="prediction",
            metricName="f1"
        )
        
        evaluator_accuracy = MulticlassClassificationEvaluator(
            labelCol="label",
            predictionCol="prediction",
            metricName="accuracy"
        )
        
        f1_score = evaluator_f1.evaluate(predictions)
        accuracy = evaluator_accuracy.evaluate(predictions)
        
        print(f"æµ‹è¯•é›† F1 åˆ†æ•°: {f1_score:.4f}")
        print(f"æµ‹è¯•é›† å‡†ç¡®ç‡: {accuracy:.4f}")
        
        print("\né¢„æµ‹ç»“æœç¤ºä¾‹:")
        predictions.select("label", "prediction", "cleaned_review").show(10, truncate=30)
        
        # ====== ä¿å­˜æ¨¡å‹ ======
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        model_save_path = "./data/spark_models/weibo_sentiment_model"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("./data/spark_models", exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        pipeline_model.write().overwrite().save(model_save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_sample = predictions.select("label", "prediction", "cleaned_review").limit(50)
        output_csv_path = "./data/pandas_processed/spark_predictions_sample.csv"
        
        os.makedirs("./data/pandas_processed", exist_ok=True)
        predictions_sample.toPandas().to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… é¢„æµ‹ç»“æœç¤ºä¾‹å·²ä¿å­˜è‡³: {output_csv_path}")
        
        # ====== åˆ›å»ºé¢„æµ‹å‡½æ•° ======
        print("\nğŸ”® åˆ›å»ºé¢„æµ‹å‡½æ•°ç¤ºä¾‹...")
        
        def predict_sentiment(text):
            """ä½¿ç”¨Sparkæ¨¡å‹é¢„æµ‹å•æ¡æ–‡æœ¬æƒ…æ„Ÿ"""
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = spark.createDataFrame([(text,)], ["review"])
            
            # åº”ç”¨ç›¸åŒçš„æ¸…æ´—æ­¥éª¤
            test_cleaned = test_data.withColumn(
                "cleaned_review",
                regexp_replace(
                    regexp_replace(
                        regexp_replace(col("review"), r'https?://\S+', ''),
                        r'@[\w\u4e00-\u9fa5]+', ''
                    ),
                    r'\[.*?\]', ''
                )
            )
            
            # åˆ†è¯
            test_segmented = test_cleaned.withColumn("words", segment_udf(col("cleaned_review")))
            
            # é¢„æµ‹
            result = pipeline_model.transform(test_segmented)
            
            # æå–ç»“æœ
            if result.count() > 0:
                pred = result.first()
                sentiment = "æ­£é¢" if pred["prediction"] == 1 else "è´Ÿé¢"
                
                # è·å–æ¦‚ç‡
                probability_vector = pred["probability"]
                if probability_vector:
                    prob = float(probability_vector[1]) if pred["prediction"] == 1 else float(probability_vector[0])
                else:
                    prob = 0.5
                    
                return sentiment, prob
            return "æœªçŸ¥", 0.0
        
        # æµ‹è¯•é¢„æµ‹å‡½æ•°
        test_texts = [
            "è¿™éƒ¨ç”µå½±çœŸçš„å¤ªå¥½çœ‹äº†ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå‰§æƒ…ä¹Ÿå¾ˆå¸å¼•äººï¼",
            "å¤ªå¤±æœ›äº†ï¼Œå®Œå…¨æµªè´¹æ—¶é—´",
            "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„"
        ]
        
        print("\næµ‹è¯•é¢„æµ‹å‡½æ•°:")
        for text in test_texts:
            sentiment, prob = predict_sentiment(text)
            print(f"  æ–‡æœ¬: {text[:30]}...")
            print(f"  æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {prob:.2%})")
            print()
        
        print("\n" + "="*60)
        print("ğŸ‰ Spark ML å…¨æµç¨‹å®ç°å®Œæˆï¼")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # åœæ­¢Sparkä¼šè¯
        if spark:
            spark.stop()
            print("\nğŸ›‘ Sparkä¼šè¯å·²åœæ­¢ã€‚")

if __name__ == "__main__":
    main()