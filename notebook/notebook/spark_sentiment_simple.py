# spark_sentiment_simple.py
import os
import sys

print("="*60)
print("ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æ - Spark ML ç®€åŒ–ç‰ˆï¼ˆé¿å…UDFé—®é¢˜ï¼‰")
print("="*60)

# ====== 1. è®¾ç½®ç¯å¢ƒå˜é‡ ======
os.environ['HADOOP_HOME'] = 'C:\\hadoop'  # å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™è®¾ä¸ºç©º
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# ====== 2. å¯¼å…¥Sparkæ¨¡å— ======
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, regexp_replace
    from pyspark.ml import Pipeline
    from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
    from pyspark.ml.classification import LogisticRegression
    from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    print("âœ… PySparkæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ PySparkå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…pyspark: pip install pyspark")
    sys.exit(1)

# ====== 3. åˆ›å»ºSparkSession ======
try:
    spark = SparkSession.builder \
        .appName("Weibo_Sentiment_Simple") \
        .master("local[*]") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
        .config("spark.python.worker.timeout", "600") \
        .getOrCreate()
    
    print("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸï¼")
    print(f"Sparkç‰ˆæœ¬: {spark.version}")
except Exception as e:
    print(f"âŒ Sparkä¼šè¯åˆ›å»ºå¤±è´¥: {e}")
    sys.exit(1)

# ====== 4. ä¸»å‡½æ•° ======
def main():
    try:
        # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆé¿å…æ–‡ä»¶è¯»å–é—®é¢˜ï¼‰
        print("\nğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        data = [
            (1, "è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆå¥½çœ‹ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå‰§æƒ…ä¹Ÿå¾ˆå¸å¼•äººï¼"),
            (0, "å¤ªå¤±æœ›äº†ï¼Œæµªè´¹æ—¶é—´å’Œé‡‘é’±ï¼Œå®Œå…¨ä¸å€¼å¾—çœ‹ã€‚"),
            (1, "å¼ºçƒˆæ¨èï¼è¿™æ˜¯æˆ‘ä»Šå¹´çœ‹è¿‡æœ€å¥½çš„ç”µå½±ï¼"),
            (0, "éå¸¸ç³Ÿç³•çš„ä½“éªŒï¼Œå¯¼æ¼”åˆ°åº•åœ¨æƒ³ä»€ä¹ˆï¼Ÿ"),
            (1, "éŸ³ä¹å¾ˆæ£’ï¼Œç”»é¢å¾ˆç¾ï¼Œå€¼å¾—ä¸€çœ‹ã€‚"),
            (0, "å‰§æƒ…æ‹–æ²“ï¼Œæ¯«æ— æ–°æ„ï¼Œæµªè´¹æ—¶é—´ã€‚"),
            (1, "æ¼”å‘˜è¡¨æ¼”å‡ºè‰²ï¼Œæ•…äº‹æƒ…èŠ‚æ„Ÿäººã€‚"),
            (0, "ç‰¹æ•ˆå¤ªå‡ï¼Œå‰§æƒ…æ¼æ´ç™¾å‡ºã€‚"),
            (1, "å¯¼æ¼”åŠŸåŠ›æ·±åšï¼Œæ¯ä¸ªç»†èŠ‚éƒ½å¾ˆåˆ°ä½ã€‚"),
            (0, "çœ‹äº†ååˆ†é’Ÿå°±æƒ³ç¦»å¼€ï¼Œå¤ªæ— èŠäº†ã€‚"),
            (1, "å‰§æƒ…åè½¬å‡ºäººæ„æ–™ï¼Œéå¸¸ç²¾å½©ã€‚"),
            (0, "è§’è‰²å¡‘é€ å¤±è´¥ï¼Œæ— æ³•å¼•èµ·å…±é¸£ã€‚"),
            (1, "è§†è§‰æ•ˆæœéœ‡æ’¼ï¼Œå€¼å¾—å»ç”µå½±é™¢è§‚çœ‹ã€‚"),
            (0, "å°è¯ç”Ÿç¡¬ï¼Œæ¼”å‘˜è¡¨æ¼”åšä½œã€‚"),
            (1, "æƒ…æ„ŸçœŸæŒšï¼Œè®©äººæ„ŸåŠ¨è½æ³ªã€‚"),
        ]
        
        columns = ["label", "review"]
        df = spark.createDataFrame(data, columns)
        print(f"åˆ›å»ºäº† {df.count()} æ¡ç¤ºä¾‹æ•°æ®")
        df.show(5)
        
        # ====== 5. æ•°æ®æ¸…æ´— ======
        print("\nğŸ”§ æ•°æ®æ¸…æ´—...")
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…æ´—
        df_cleaned = df.withColumn(
            "cleaned_review",
            regexp_replace(
                regexp_replace(
                    regexp_replace(
                        col("review"),
                        r'https?://\S+', ''
                    ),
                    r'@\w+', ''
                ),
                r'\[.*?\]', ''
            )
        )
        
        df_cleaned = df_cleaned.withColumn(
            "cleaned_review",
            regexp_replace(col("cleaned_review"), r'[^\w\u4e00-\u9fa5\s]', '')
        )
        
        print("æ¸…æ´—åæ•°æ®ç¤ºä¾‹:")
        df_cleaned.select("review", "cleaned_review").show(3, truncate=False)
        
        # ====== 6. ä½¿ç”¨Sparkå†…ç½®Tokenizerï¼ˆé¿å…UDFï¼‰ ======
        print("\nğŸ”ª æ–‡æœ¬åˆ†è¯ï¼ˆä½¿ç”¨Sparkå†…ç½®Tokenizerï¼‰...")
        
        # Sparkå†…ç½®çš„Tokenizeræ˜¯æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œå¯¹äºä¸­æ–‡æ•ˆæœæœ‰é™
        # ä½†è¿™æ˜¯æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ä¼šè§¦å‘Python workeré—®é¢˜
        tokenizer = Tokenizer(inputCol="cleaned_review", outputCol="words")
        
        # åº”ç”¨åˆ†è¯
        df_tokenized = tokenizer.transform(df_cleaned)
        print("åˆ†è¯ç»“æœç¤ºä¾‹:")
        df_tokenized.select("cleaned_review", "words").show(3, truncate=False)
        
        # ====== 7. åœç”¨è¯å¤„ç† ======
        print("\nğŸ“– åœç”¨è¯è¿‡æ»¤...")
        # ç®€å•çš„åœç”¨è¯åˆ—è¡¨
        stop_words = ["çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", 
                     "ä¸", "äºº", "éƒ½", "ä¹Ÿ", "è€Œ", "åŠ", "ä¸", "ç€", 
                     "æˆ–", "ä¸ª", "æ²¡æœ‰", "è¿™", "é‚£", "å°±", "ä¹Ÿ", "å¾ˆ"]
        
        remover = StopWordsRemover(
            inputCol="words",
            outputCol="filtered_words",
            stopWords=stop_words
        )
        
        df_filtered = remover.transform(df_tokenized)
        
        # ====== 8. ç‰¹å¾å·¥ç¨‹ ======
        print("\nâš™ï¸ ç‰¹å¾å·¥ç¨‹...")
        
        hashing_tf = HashingTF(
            inputCol="filtered_words",
            outputCol="raw_features",
            numFeatures=100
        )
        
        idf = IDF(
            inputCol="raw_features",
            outputCol="features",
            minDocFreq=1
        )
        
        # ====== 9. æ¨¡å‹è®­ç»ƒ ======
        print("\nğŸ¤– è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="label",
            maxIter=10,
            regParam=0.1
        )
        
        # æ„å»ºPipeline
        pipeline = Pipeline(stages=[
            tokenizer,
            remover,
            hashing_tf,
            idf,
            lr
        ])
        
        # åˆ’åˆ†æ•°æ®é›†
        train_df, test_df = df_filtered.randomSplit([0.7, 0.3], seed=42)
        print(f"è®­ç»ƒé›†: {train_df.count()} æ¡")
        print(f"æµ‹è¯•é›†: {test_df.count()} æ¡")
        
        # è®­ç»ƒæ¨¡å‹
        model = pipeline.fit(train_df)
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # ====== 10. æ¨¡å‹è¯„ä¼° ======
        print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
        predictions = model.transform(test_df)
        
        # è¯„ä¼°æŒ‡æ ‡
        evaluator = MulticlassClassificationEvaluator(
            labelCol="label",
            predictionCol="prediction",
            metricName="accuracy"
        )
        
        accuracy = evaluator.evaluate(predictions)
        print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
        
        print("\né¢„æµ‹ç»“æœ:")
        predictions.select("label", "prediction", "review").show(10, truncate=False)
        
        # ====== 11. ä¿å­˜ç»“æœ ======
        print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("./data/spark_results", exist_ok=True)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions.select("label", "prediction", "review").toPandas().to_csv(
            "./data/spark_results/simple_predictions.csv", 
            index=False, 
            encoding='utf-8-sig'
        )
        
        # ä¿å­˜æ¨¡å‹æŠ¥å‘Š
        with open("./data/spark_results/simple_model_report.txt", "w", encoding="utf-8") as f:
            f.write("Sparkç®€åŒ–ç‰ˆæƒ…æ„Ÿåˆ†ææ¨¡å‹æŠ¥å‘Š\n")
            f.write("="*50 + "\n")
            f.write(f"æ•°æ®é‡: {df.count()} æ¡\n")
            f.write(f"å‡†ç¡®ç‡: {accuracy:.4f}\n")
            f.write("æ¨¡å‹: é€»è¾‘å›å½’\n")
            f.write("ç‰¹å¾: HashingTF + IDF\n")
        
        print("âœ… ç»“æœå·²ä¿å­˜åˆ° ./data/spark_results/")
        
        # ====== 12. æ¼”ç¤ºé¢„æµ‹åŠŸèƒ½ ======
        print("\nğŸ”® æ¼”ç¤ºé¢„æµ‹åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_samples = [
            ("è¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’",),
            ("å¤ªç³Ÿç³•äº†ï¼Œå®Œå…¨æµªè´¹æ—¶é—´",),
            ("ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«",)
        ]
        
        test_df_demo = spark.createDataFrame(test_samples, ["review"])
        
        # å¯¹æµ‹è¯•æ•°æ®åº”ç”¨ç›¸åŒçš„é¢„å¤„ç†
        test_cleaned = test_df_demo.withColumn(
            "cleaned_review",
            regexp_replace(col("review"), r'[^\w\u4e00-\u9fa5\s]', '')
        )
        
        # è¿›è¡Œé¢„æµ‹
        test_predictions = model.transform(test_cleaned)
        
        print("æµ‹è¯•é¢„æµ‹ç»“æœ:")
        for row in test_predictions.collect():
            sentiment = "æ­£é¢" if row.prediction == 1 else "è´Ÿé¢"
            print(f"  æ–‡æœ¬: {row.review}")
            print(f"  é¢„æµ‹: {sentiment}")
            print()
        
        print("\n" + "="*60)
        print("âœ… Sparkç®€åŒ–ç‰ˆè¿è¡ŒæˆåŠŸï¼")
        print("="*60)
        print("\næ³¨æ„ï¼šç”±äºWindowsä¸Šçš„Sparké™åˆ¶ï¼Œæ­¤ç‰ˆæœ¬:")
        print("1. ä½¿ç”¨Sparkå†…ç½®Tokenizerï¼ˆæŒ‰ç©ºæ ¼åˆ†è¯ï¼‰ï¼Œä¸­æ–‡åˆ†è¯æ•ˆæœæœ‰é™")
        print("2. ä½¿ç”¨å°è§„æ¨¡ç¤ºä¾‹æ•°æ®")
        print("3. é¿å…äº†å¯èƒ½å¼•å‘é—®é¢˜çš„Python UDF")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# ====== è¿è¡Œä¸»ç¨‹åº ======
if __name__ == "__main__":
    success = main()
    
    # åœæ­¢Sparkä¼šè¯
    if 'spark' in locals():
        spark.stop()
        print("\nğŸ›‘ Sparkä¼šè¯å·²åœæ­¢ã€‚")
    
    if not success:
        print("\nâš ï¸  è¿è¡Œå¤±è´¥ï¼Œè¯·å°è¯•ä»¥ä¸‹æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨Pandas/Scikit-learnç‰ˆæœ¬ï¼ˆæ¨èï¼‰")
        print("2. åœ¨Linux/WSLç¯å¢ƒä¸­è¿è¡ŒSpark")
        print("3. ä½¿ç”¨äº‘æœåŠ¡è¿è¡ŒSpark")